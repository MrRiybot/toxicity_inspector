{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc52bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import pickle\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c414c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_path,tokenizer_path):\n",
    "    arabic_model = load_model(model_path)\n",
    "\n",
    "    # loading\n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        arabic_tokenizer = pickle.load(handle)\n",
    "    \n",
    "    return arabic_model,arabic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163c57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model,arabic_tokenizer = get_model('arabic_model.h5','arabic_tokenizer.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e702a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_arabic(inp,model,tokenizer):\n",
    "    text_token = tokenizer.texts_to_sequences(inp)\n",
    "    maxlen = 13 # choosed by the avg\n",
    "    text_token_pad = pad_sequences(text_token, maxlen=maxlen)\n",
    "\n",
    "    o = model.predict(text_token_pad)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a31c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_english(inp,model,tokenizer):\n",
    "    text_token = tokenizer.texts_to_sequences(inp)\n",
    "    maxlen = 200 # choosed by the avg\n",
    "    text_token_pad = pad_sequences(text_token, maxlen=maxlen)\n",
    "\n",
    "    o = model.predict(text_token_pad)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77312e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E004] Can't set up pipeline component: a factory for 'language_detector' already exists. Existing factory: <function language_detector at 0x0000025B789F2160>. New factory: <function language_detector at 0x0000025B0AD46A60>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      4\u001b[0m \u001b[38;5;129;43m@Language\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage_detector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mlanguage_detector\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLanguageDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_langauge_model\u001b[39m():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\projects\\lib\\site-packages\\spacy\\language.py:489\u001b[0m, in \u001b[0;36mLanguage.factory.<locals>.add_factory\u001b[1;34m(factory_func)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m util\u001b[38;5;241m.\u001b[39mis_same_func(factory_func, existing_func):\n\u001b[0;32m    486\u001b[0m         err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE004\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    487\u001b[0m             name\u001b[38;5;241m=\u001b[39mname, func\u001b[38;5;241m=\u001b[39mexisting_func, new_func\u001b[38;5;241m=\u001b[39mfactory_func\n\u001b[0;32m    488\u001b[0m         )\n\u001b[1;32m--> 489\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    491\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_arg_names(factory_func)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_names \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_names:\n",
      "\u001b[1;31mValueError\u001b[0m: [E004] Can't set up pipeline component: a factory for 'language_detector' already exists. Existing factory: <function language_detector at 0x0000025B789F2160>. New factory: <function language_detector at 0x0000025B0AD46A60>"
     ]
    }
   ],
   "source": [
    "from spacy_langdetect import LanguageDetector\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "@Language.factory('language_detector')\n",
    "def language_detector(nlp, name):\n",
    "    return LanguageDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c276ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langauge_model():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 2000000\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36440c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_langauge(text,nlp): \n",
    "    doc = nlp(text)\n",
    "    detect_language = doc._.language\n",
    "    return detect_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "493ab96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model,arabic_tokenizer = get_model('arabic_model.h5','arabic_tokenizer.pickle')\n",
    "english_model,english_tokenizer = get_model('english_model.h5','english_tokenizer.pickle')\n",
    "nlp = get_langauge_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b55b8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(text,arabic_model,arabic_tokenizer,english_model,english_tokenizer,nlp):\n",
    "    if predict_langauge(text,nlp)['language'] == 'en':\n",
    "        return 'en',predict_english([text],english_model,english_tokenizer)[0][0]\n",
    "    elif predict_langauge(text,nlp)['language'] in ['ar','fa','ur']:\n",
    "        return 'ar',predict_arabic([text],arabic_model,arabic_tokenizer)[0][0] \n",
    "    else:\n",
    "        return -1,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "297edd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_toxicity(text):\n",
    "    lang,pred = predict_toxicity(text,arabic_model,arabic_tokenizer,english_model,english_tokenizer,nlp)\n",
    "    if(lang==-1):\n",
    "        print(\"model does not take other lang than arabic,english\")\n",
    "        return None\n",
    "    if(lang=='en'):\n",
    "        print(\"text is english\")\n",
    "    elif (lang=='ar'):\n",
    "        print('text is arabic')\n",
    "    if(pred>=0.5):\n",
    "        print(\"text is TOXIC\")\n",
    "    else:\n",
    "        print(\"text is not TOXIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "990eab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 228ms/step\n",
      "text is arabic\n",
      "text is TOXIC\n"
     ]
    }
   ],
   "source": [
    "display_toxicity('يا غبي')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33e8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
